{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T12:40:17.872949Z","iopub.status.busy":"2024-04-28T12:40:17.872555Z","iopub.status.idle":"2024-04-28T12:40:35.601794Z","shell.execute_reply":"2024-04-28T12:40:35.600605Z","shell.execute_reply.started":"2024-04-28T12:40:17.872916Z"},"trusted":true},"outputs":[],"source":["%%capture\n","%pip install gdown\n","!gdown --fuzzy https://drive.google.com/file/d/16M6_lcY-rDcOV3uMeNt8Ukg9A6E-T6Cv/view?usp=sharing\n","!unzip -o NLP3.zip\n","!rm NLP3.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T12:40:35.604305Z","iopub.status.busy":"2024-04-28T12:40:35.603978Z","iopub.status.idle":"2024-04-28T12:40:35.610565Z","shell.execute_reply":"2024-04-28T12:40:35.609618Z","shell.execute_reply.started":"2024-04-28T12:40:35.604277Z"},"trusted":true},"outputs":[],"source":["import itertools\n","\n","from transformers import (\n","    AutoModelForTokenClassification,\n","    AutoTokenizer,\n","    DataCollatorForTokenClassification,\n","    Trainer,\n","    TrainingArguments,\n","    pipeline,\n",")\n","from datasets import Dataset\n","from torchmetrics.functional import f1_score\n","from transformers.pipelines.pt_utils import KeyDataset\n","import pandas as pd\n","import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T12:40:35.612308Z","iopub.status.busy":"2024-04-28T12:40:35.611908Z","iopub.status.idle":"2024-04-28T12:40:35.674464Z","shell.execute_reply":"2024-04-28T12:40:35.673705Z","shell.execute_reply.started":"2024-04-28T12:40:35.612273Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_json('train.jsonl', lines=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T12:40:35.676698Z","iopub.status.busy":"2024-04-28T12:40:35.676414Z","iopub.status.idle":"2024-04-28T12:40:35.686689Z","shell.execute_reply":"2024-04-28T12:40:35.685787Z","shell.execute_reply.started":"2024-04-28T12:40:35.676674Z"},"trusted":true},"outputs":[],"source":["id2label = dict(enumerate(['O'] + sorted({part for ners in train_df.ners for _, _, part in ners})))\n","label2id = {label: idx for idx, label in id2label.items()}\n","print(len(id2label), 'labels in total')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T12:40:35.688173Z","iopub.status.busy":"2024-04-28T12:40:35.687804Z","iopub.status.idle":"2024-04-28T12:40:35.697680Z","shell.execute_reply":"2024-04-28T12:40:35.696860Z","shell.execute_reply.started":"2024-04-28T12:40:35.688141Z"},"trusted":true},"outputs":[],"source":["model_id = 'sentence-transformers/LaBSE'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T12:40:35.699086Z","iopub.status.busy":"2024-04-28T12:40:35.698739Z","iopub.status.idle":"2024-04-28T12:40:42.112777Z","shell.execute_reply":"2024-04-28T12:40:42.111951Z","shell.execute_reply.started":"2024-04-28T12:40:35.699045Z"},"trusted":true},"outputs":[],"source":["model = AutoModelForTokenClassification.from_pretrained(model_id, \n","                                                        num_labels=len(id2label), \n","                                                        ignore_mismatched_sizes=True,\n","                                                        id2label=id2label,\n","                                                        label2id=label2id)\n","tokenizer = AutoTokenizer.from_pretrained(model_id, add_prefix_space=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T12:40:42.177414Z","iopub.status.busy":"2024-04-28T12:40:42.177058Z","iopub.status.idle":"2024-04-28T12:40:42.182938Z","shell.execute_reply":"2024-04-28T12:40:42.182036Z","shell.execute_reply.started":"2024-04-28T12:40:42.177385Z"},"trusted":true},"outputs":[],"source":["def convert_dataframe(dataframe):\n","    dataframe = dataframe.copy()\n","    def convert_ners(ners):\n","        return [(start, end, label2id[label]) for start, end, label in ners]\n","    dataframe['ners'] = dataframe.ners.apply(convert_ners)\n","    return dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T12:40:42.184412Z","iopub.status.busy":"2024-04-28T12:40:42.184142Z","iopub.status.idle":"2024-04-28T12:40:42.242771Z","shell.execute_reply":"2024-04-28T12:40:42.242045Z","shell.execute_reply.started":"2024-04-28T12:40:42.184380Z"},"trusted":true},"outputs":[],"source":["train_set = Dataset.from_pandas(convert_dataframe(train_df))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T12:40:42.244512Z","iopub.status.busy":"2024-04-28T12:40:42.243977Z","iopub.status.idle":"2024-04-28T12:40:42.254192Z","shell.execute_reply":"2024-04-28T12:40:42.253187Z","shell.execute_reply.started":"2024-04-28T12:40:42.244479Z"},"trusted":true},"outputs":[],"source":["class DatasetTokenizer:\n","    \"\"\"Tokenizer for the dataset.\n","\n","    Splits sentences in the dataset into tokens using provided\n","    HuggingFace tokenizer, and labels the tokens.\n","    \"\"\"\n","\n","    def __init__(self, tokenizer):\n","        \"\"\"Initializes the tokenizer.\n","\n","        Args:\n","            tokenizer (Tokenizer): HuggingFace tokenizer.\n","        \"\"\"\n","        self.tokenizer = tokenizer\n","\n","    def __call__(self, row: dict) -> dict:\n","        \"\"\"Splits sentences into tokens and labels them.\n","\n","        Args:\n","            row (dict): Row in the dataset. Should contain the following keys:\n","                sentences (str): text to tokenize\n","                ners (list[tuple[int, int, str]]): Labels for words as tuples\n","                    of (start, end, label).\n","        \n","        Returns:\n","            dict: Tokenized row. Contains:\n","                input_ids (list[int]): IDs of the tokens in sentences.\n","                labels (list[int]): List of label IDs corresponding to\n","                    tokens.\n","                token_type_ids (list[int]): List of token type IDs.\n","                    See HuggingFace documentation on tokenizers for\n","                    further detail.\n","                attention_mask (list[int]): Whether to attend to tokens\n","                    or not. See HuggingFace documentation for further\n","                    detail.\n","        \"\"\"\n","        text_len = len(row['sentences'])\n","        char_labels = [label2id['O'] for _ in range(text_len)]\n","        for start, end, label in row['ners']:\n","            for i in range(start, end):\n","                char_labels[i] = label\n","        tokenized = tokenizer(row['sentences'])\n","        n_tokens = len(tokenized['input_ids'])\n","        labels = [label2id['O']] * n_tokens\n","        for i in range(n_tokens):\n","            span = tokenized.token_to_chars(i)\n","            if span is None:\n","                continue\n","            labels[i] = char_labels[span.start]\n","        tokenized['labels'] = labels\n","        return tokenized"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T12:40:42.257288Z","iopub.status.busy":"2024-04-28T12:40:42.257030Z","iopub.status.idle":"2024-04-28T12:40:42.268719Z","shell.execute_reply":"2024-04-28T12:40:42.267878Z","shell.execute_reply.started":"2024-04-28T12:40:42.257266Z"},"trusted":true},"outputs":[],"source":["def split_into_multiple(batch):\n","    \"\"\"Splits one row into multiple to prevent overflowing the model's\n","    context window.\n","\n","    Args:\n","        batch (dict[str, list]): Batch of dataset rows. Should contain column\n","            `labels`, and every other column should be the same length.\n","    \"\"\"\n","    result = {column: [] for column in batch}\n","    batch_size = len(batch['labels'])\n","    for i in range(batch_size):\n","        n = len(batch['labels'][i])\n","        for j in itertools.count(0, 384):\n","            for column in batch:\n","                result[column].append(batch[column][i][j:j+512])\n","            if j + 512 >= n:\n","                break\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T12:40:42.270027Z","iopub.status.busy":"2024-04-28T12:40:42.269749Z","iopub.status.idle":"2024-04-28T12:40:42.280775Z","shell.execute_reply":"2024-04-28T12:40:42.280057Z","shell.execute_reply.started":"2024-04-28T12:40:42.270005Z"},"trusted":true},"outputs":[],"source":["dataset_tokenizer = DatasetTokenizer(tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T12:41:09.779556Z","iopub.status.busy":"2024-04-28T12:41:09.779167Z","iopub.status.idle":"2024-04-28T12:41:12.250444Z","shell.execute_reply":"2024-04-28T12:41:12.249557Z","shell.execute_reply.started":"2024-04-28T12:41:09.779526Z"},"trusted":true},"outputs":[],"source":["dataset = train_set\\\n","    .map(dataset_tokenizer)\\\n","    .remove_columns(['id', 'sentences', 'ners'])\\\n","    .map(split_into_multiple, batched=True)\\\n","    .train_test_split(test_size=.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T12:41:13.405879Z","iopub.status.busy":"2024-04-28T12:41:13.405488Z","iopub.status.idle":"2024-04-28T12:41:13.411938Z","shell.execute_reply":"2024-04-28T12:41:13.410958Z","shell.execute_reply.started":"2024-04-28T12:41:13.405818Z"},"trusted":true},"outputs":[],"source":["def compute_metrics(results) -> dict[str, float]:\n","    \"\"\"Computes the metrics for the model's evaluation.\n","\n","    Args:\n","        results: Outputs of the model.\n","    \n","    Returns:\n","        dict[str, float]: Mapping from the name of the metric to its value.\n","    \"\"\"\n","    preds, target = results\n","    f1 = f1_score(torch.tensor(preds).transpose(-1, -2),\n","                  torch.tensor(target),\n","                  num_classes=len(id2label),\n","                  average='macro',\n","                  task='multiclass',\n","                  ignore_index=-100)\n","    return {'f1_score': f1}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T12:41:13.970008Z","iopub.status.busy":"2024-04-28T12:41:13.969184Z","iopub.status.idle":"2024-04-28T12:41:14.703153Z","shell.execute_reply":"2024-04-28T12:41:14.702305Z","shell.execute_reply.started":"2024-04-28T12:41:13.969976Z"},"trusted":true},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir=\"/tmp\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=20,\n","    save_total_limit = 4,\n","    weight_decay=0.01,\n","    report_to='tensorboard',\n","    save_strategy='epoch',\n","    load_best_model_at_end=True,\n",")\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dataset['train'],\n","    eval_dataset=dataset['test'],\n","    data_collator=DataCollatorForTokenClassification(tokenizer),\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T12:41:14.705203Z","iopub.status.busy":"2024-04-28T12:41:14.704914Z","iopub.status.idle":"2024-04-28T12:42:00.881566Z","shell.execute_reply":"2024-04-28T12:42:00.880116Z","shell.execute_reply.started":"2024-04-28T12:41:14.705178Z"},"trusted":true},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-28T12:42:00.883296Z","iopub.status.idle":"2024-04-28T12:42:00.883657Z","shell.execute_reply":"2024-04-28T12:42:00.883491Z","shell.execute_reply.started":"2024-04-28T12:42:00.883476Z"},"trusted":true},"outputs":[],"source":["trainer.model.save_pretrained('./model')\n","tokenizer.save_pretrained('./model')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-28T12:40:44.377360Z","iopub.status.idle":"2024-04-28T12:40:44.377693Z","shell.execute_reply":"2024-04-28T12:40:44.377546Z","shell.execute_reply.started":"2024-04-28T12:40:44.377531Z"},"trusted":true},"outputs":[],"source":["!zip -r model.zip -xi model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-28T12:40:44.378996Z","iopub.status.idle":"2024-04-28T12:40:44.379376Z","shell.execute_reply":"2024-04-28T12:40:44.379209Z","shell.execute_reply.started":"2024-04-28T12:40:44.379193Z"},"trusted":true},"outputs":[],"source":["!rm *.jsonl\n","!rm -r model\n","!rm *.json"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
